{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "This example is derived from Udemy Machine Learning A-Z course. It is intended to show how to eliminate unsignificant features in Multiple Linear Regression, based on *P-value* and *adjusted R Square*. \n",
    "\n",
    "*P-value* and *adjusted R Square* are calculated using the **statsmodel** python library.\n",
    "\n",
    "\n",
    "P Value is useful to decide which features have significant influence on prediction results. But for those features which have P value close to SL, it is sometimes tricky to decide to keep or not. Thus, an improved method to consider also the adjusted R square is introduced. Ajust R square indicates the \"Goodness of Fitting\". If removing the feature which has slightly higher P value than threshold does not help to improve the ajusted R square, it is better not to remove such features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow multiple outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas.core import datetools\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Set the numpy print format\n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardEliminationWithPvalueOnly(X, y, SL):\n",
    "    \"\"\"\n",
    "    X is feature column(s)\n",
    "    y in the dependent variable\n",
    "    SL is the significant level threshold, feature with p-value higher than SL should be abandoned\n",
    "    \"\"\"\n",
    "    num_of_features = X.shape[1]\n",
    "    for i in range(num_of_features):\n",
    "        regressor_OLS = sm.OLS(y, X).fit()\n",
    "        print (regressor_OLS.summary())\n",
    "        max_pvalue = max(regressor_OLS.pvalues).astype(float)\n",
    "        max_pvalue_pos = np.argmax(regressor_OLS.pvalues)\n",
    "        if max_pvalue > SL:\n",
    "            X = np.delete(X, max_pvalue_pos, axis=1) # delete the j column with max pvalue\n",
    "        else:\n",
    "            break\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardEliminationWithRsquare(X, y, SL):\n",
    "    \"\"\"\n",
    "    X is feature column(s)\n",
    "    y in the dependent variable\n",
    "    SL is the significant level threshold, feature with p-value higher than SL should be abandoned\n",
    "    \"\"\"\n",
    "    num_of_features = X.shape[1]\n",
    "    for i in range(num_of_features):\n",
    "        regressor_OLS = sm.OLS(y, X).fit()\n",
    "        print (regressor_OLS.summary())\n",
    "        max_pvalue = max(regressor_OLS.pvalues).astype(float)\n",
    "        max_pvalue_pos = np.argmax(regressor_OLS.pvalues)\n",
    "        _adjRSqare_before = regressor_OLS.rsquared_adj.astype(float)\n",
    "        if max_pvalue > SL:\n",
    "            # Check with a copy if the adjusted R square will improve after removing the feature\n",
    "            # if imporoves, delete the real feature, otherwise continue\n",
    "            X_copy =X.copy()\n",
    "            X_copy = np.delete(X_copy, max_pvalue_pos, axis=1)\n",
    "            regressor_temp = sm.OLS(y, X_copy).fit()\n",
    "            _adjRSqare_after = regressor_temp.rsquared_adj.astype(float)\n",
    "            \n",
    "            if  _adjRSqare_after > _adjRSqare_before:\n",
    "                X = np.delete(X, max_pvalue_pos, axis=1) # delete the j column with max pvalue\n",
    "        else:\n",
    "            # If the max_pvalue is less than threslhold, all significant features are found             \n",
    "            break\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Backward Elimination logic\n",
    "\n",
    "def backwardElimination(X, y, SL):\n",
    "    \"\"\"\n",
    "    X is feature column(s)\n",
    "    y in the dependent variable\n",
    "    SL is the significant level threshold, feature with p-value higher than SL should be abandoned\n",
    "    \"\"\"\n",
    "    _Xshape = X.shape\n",
    "    num_of_features = _Xshape[1]\n",
    "    temp = np.zeros(_Xshape).astype(int)\n",
    "    \n",
    "    for i in range(0, num_of_features):\n",
    "        regressor_OLS = sm.OLS(y, X).fit()\n",
    "        print (regressor_OLS.summary())\n",
    "        max_pvalue = max(regressor_OLS.pvalues).astype(float)\n",
    "        adjR_before = regressor_OLS.rsquared_adj.astype(float)\n",
    "        if max_pvalue > SL:\n",
    "            for j in range(0, num_of_features - i):\n",
    "                if (regressor_OLS.pvalues[j].astype(float) == max_pvalue):\n",
    "                    temp[:,j] = X[:, j]\n",
    "                    X = np.delete(X, j, 1)\n",
    "                    tmp_regressor = sm.OLS(y, X).fit()\n",
    "                    adjR_after = tmp_regressor.rsquared_adj.astype(float)\n",
    "                    print (tmp_regressor.summary())\n",
    "                    if (adjR_before >= adjR_after):\n",
    "                        X_rollback = np.hstack((X, temp[:,[0,j]]))\n",
    "                        X_rollback = np.delete(X_rollback, j, 1)\n",
    "                        print (regressor_OLS.summary())\n",
    "                        return X_rollback\n",
    "                    else:\n",
    "                        continue\n",
    "    regressor_OLS.summary()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('50_Startups.csv')\n",
    "dataset.head()\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "X[:, 3] = labelencoder.fit_transform(X[:, 3])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [3])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "# Avoiding the Dummy Variable Trap, remove redudant column\n",
    "print(\"with dummy variabe\")\n",
    "X[:3]\n",
    "X = X[:, 1:]\n",
    "print(\"without dummy variabe\")\n",
    "X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Multiple Linear Regression to the Training set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get the significant features considering only P value\n",
    "X_with_ones=sm.add_constant(X)\n",
    "X_return = backwardEliminationWithPvalueOnly(X=X_with_ones, y=y, SL=0.05)\n",
    "X_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get the significant features considering P value and adjusted R sqaure\n",
    "# Method 1\n",
    "X_return = backwardEliminationWithRsquare(X=X_with_ones, y=y, SL=0.05)\n",
    "X_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get the significant features considering P value and adjusted R sqaure\n",
    "# Method 2\n",
    "X_return = backwardElimination(X=X_with_ones, y=y, SL=0.05)\n",
    "X_return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
